#!/bin/bash

##############################################################################################
# Intended to run hourly. Checks whether all WARCs created in the last 2 hours exist in HDFS #
# at the same location.                                                                      #
# If not, they are copied and a MapReduce job run to CDX-index them.                         #
# A full CDX can be created by sorting all the outputs.                                      #
##############################################################################################

HDFS_CDX_ROOT=/heritrix/output/wayback/cdx-index
ROOT=/heritrix/output
HOOP="http://opera.bl.uk:14000/webhdfs/v1"
SUFFIX="?user.name=hadoop&op=GETFILESTATUS"
TMP=/tmp
JAR=/heritrix/bin/HadoopUtils.jar
CLASS=uk.bl.wap.hadoop.mapreduce.cdx.ArchiveCDXGenerator
SPLIT=/heritrix/output/wayback/split.txt
RED=260

log()
{
        echo "[$(date "+%Y-%m-%d %H:%M:%S")] $1"
}

COUNT=0
OUTPUT="$(date +%Y%m%d%H%M%S)"
while read warc
do
	TEST="$(curl --silent "$HOOP$warc$SUFFIX")"
	if [[ "$TEST" =~ "FileNotFoundException" ]]
	then
		log "Copying $warc to HDFS..."
		hadoop fs -put "$warc" "$warc"
		[[ "$warc" =~ "viral" ]] || echo "$warc" >> $TMP/$OUTPUT.job
		COUNT=$((COUNT+1))
	fi
done < <(find $ROOT/ -mtime -2 -name "*.warc.gz")

if [[ -e $TMP/$OUTPUT.job ]] && [[ $(wc -l $TMP/$OUTPUT.job | awk '{ print $1 }') -gt 0 ]]
then
	log "Generating CDX for $OUTPUT.job in $HDFS_CDX_ROOT/$OUTPUT/..."
	hadoop fs -put $TMP/$OUTPUT.job $TMP/$OUTPUT.job && rm $TMP/$OUTPUT.job
	hadoop jar $JAR $CLASS -w -i $TMP/$OUTPUT.job -o $HDFS_CDX_ROOT/$OUTPUT/ -s $SPLIT -h -r $RED &
fi

if [[ $COUNT -eq 0 ]]
then
	exit 1
else
	log "Complete. $COUNT files copied."
	exit 0
fi
