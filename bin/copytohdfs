#!/bin/bash

################
# List of jobs #
################
JOB=$(cat /home/heritrix/bin/job.name)
INPUT=/home/heritrix/bin/jobs.list
JOBS=($(cat $INPUT))

HDFS_ROOT=/crawls
ROOT=/heritrix/output/warcs
HOOP="http://opera.bl.uk:14000/webhdfs/v1"
SUFFIX="?user.name=hadoop&op=GETFILESTATUS"
TMP=/tmp
JAR=/home/rcoram/HadoopUtils/HadoopUtils.jar
CLASS=uk.bl.wap.hadoop.mapreduce.cdx.ArchiveCDXGenerator
RED=260

COUNT=0
echo "[$(date "+%Y-%m-%d %H:%M:%S")] Starting copy to HDFS..."
for job in "${JOBS[@]}"
do
	OUTPUT="$(date +%Y%m%d%H%M%S)"
	while read warc
	do
		TEST="$(curl --silent "$HOOP$HDFS_ROOT/$JOB/$job/warcs/$(basename $warc)$SUFFIX")"
		if [[ $TEST =~ "FileNotFoundException" ]]
		then
			echo -n "[$(date "+%Y-%m-%d %H:%M:%S")] Writing $HDFS_ROOT/$JOB/$job/warcs/$(basename $warc)..."
			hadoop fs -put "$warc" "$HDFS_ROOT/$JOB/$job/warcs/$(basename $warc)"
			echo "OK"
			echo "$HDFS_ROOT/$JOB/$job/warcs/$(basename $warc)" >> $TMP/$OUTPUT.job
			COUNT=$((COUNT+1))
		fi
	done < <(find $ROOT/$job/ -mtime -2 -name "*.warc.gz")

	if [[ -e $TMP/$OUTPUT.job ]] && [[ $(wc -l $TMP/$OUTPUT.job | awk '{ print $1 }') -gt 0 ]]
	then
		echo -n "[$(date "+%Y-%m-%d %H:%M:%S")] Generating CDX for $OUTPUT.job in $HDFS_ROOT/CDX/$job/$OUTPUT/..."
		hadoop fs -put $TMP/$OUTPUT.job $TMP/$OUTPUT.job && rm $TMP/$OUTPUT.job
		hadoop jar $JAR $CLASS -i $TMP/$OUTPUT.job -o $HDFS_ROOT/CDX/$job/$OUTPUT/ -s /tmp/split.txt -h -r $RED &
	fi
done
echo "[$(date "+%Y-%m-%d %H:%M:%S")] Complete. $COUNT files copied."
if [[ $COUNT -eq 0 ]]
then
	exit 1
else
	exit 0
fi
