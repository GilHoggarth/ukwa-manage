import os
import csv
import json
import gzip
import pysolr
import shutil
import logging
import datetime
import papermill as pm
import luigi.contrib.webhdfs
from prometheus_client import CollectorRegistry, Gauge
from tasks.common import state_file
from lib.targets import CrawlPackageTarget, CrawlReportTarget, ReportTarget
from tasks.ingest.list_hdfs_content import CopyFileListToHDFS
from lib.webhdfs import webhdfs
from lib.targets import AccessTaskDBTarget, DatedStateFileTask


logger = logging.getLogger('luigi-interface')

"""
Tasks relating to listing HDFS content and reporting on it.
"""


class CurrentHDFSFileList(luigi.ExternalTask):
    """
    This is the file on HDFS to look for, generated by an independent task:
    """
    date = luigi.DateParameter(default=datetime.date.today())
    task_namespace = 'analyse.hdfs'

    def output(self):
        t = CopyFileListToHDFS(self.date).output()
        logger.info("Looking for %s on HDFS..." % t.path)
        return t


class DownloadHDFSFileList(DatedStateFileTask):
    """
    This downloads the HDFS file to a local copy for processing.
    """
    date = luigi.DateParameter(default=datetime.date.today())

    task_namespace = "analyse.hdfs"

    tag = 'hdfs'
    name = 'all-files-list'
    output_ext = 'csv'
    dated_ext = 'csv.gz'

    def requires(self):
        return CurrentHDFSFileList(self.date)

    def complete(self):
        # Check the dated file exists
        dated_target = self.dated_state_file()
        logger.info("Checking %s exists..." % dated_target.path)
        exists = dated_target.exists()
        logger.info("Got %s exists = %s..." % (dated_target.path, exists))
        if not exists:
            return False
        return True

    def run(self):
        # Use Luigi's helper to ensure the dated file only appears when all is well:
        with self.dated_state_file().temporary_path() as temp_output_path:

            # Download the file to the dated, compressed file (at a temporary path):
            logger.info("Downloading %s" % self.dated_state_file().path)
            logger.info("Using temp path %s" % temp_output_path)
            client = webhdfs()
            with client.read(self.input().path) as f_in, open(temp_output_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
            logger.info("Downloaded %s" % self.dated_state_file().path)
            logger.info("Using temp path %s" % temp_output_path)

            # Also make an uncompressed version:
            logger.info("Decompressing %s" % self.dated_state_file().path)
            logger.info("Using temp path %s" % temp_output_path)
            with gzip.open(temp_output_path, 'rb') as f_in, open(self.output().path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
            logger.info("Decompressed %s" % self.dated_state_file().path)
            logger.info("Using temp path %s" % temp_output_path)


class ListParsedPaths(DatedStateFileTask):
    """
    Identifies in the files on HDFS and classifies them appropriately
    """
    date = luigi.DateParameter(default=datetime.date.today())

    task_namespace = "analyse.hdfs"

    tag = 'hdfs'
    name = 'parsed-paths'
    output_ext = 'csv'
    dated_ext = 'ipynb'

    def requires(self):
        return DownloadHDFSFileList(self.date)

    def run(self):
        # Change into this directory:
        dir_path = os.path.dirname(os.path.realpath(__file__))
        os.chdir(dir_path)
        # Record a dated flag file to show the work is done.
        with self.dated_state_file().temporary_path() as dated_out_path:
            # Perform this analysis by running the appropriate notebook:
            pm.execute_notebook(
                './hdfs_path_parser.ipynb',
                dated_out_path,
                parameters = dict(
                    file_list_csv = self.input().path,
                    parsed_files_csv = self.output().path
                )
            )


class UpdateWarcsDatabase(luigi.Task):
    """
    Lists the WARCS and arranges them by date:
    """
    date = luigi.DateParameter(default=datetime.date.today())
    trackdb = luigi.Parameter(default='http://localhost:8983/solr/tracking')
    clear_trackdb = luigi.BoolParameter(default=False) # Should only be use in testing as this will delete downstream state.

    task_namespace = 'analyse.hdfs'

    total = 0
    batch_size = 5000

    def requires(self):
        return ListParsedPaths(self.date)

    def output(self):
        return AccessTaskDBTarget(self.task_namespace, self.task_id)

    def entry_generator(self, reader):
        refresh_date = datetime.datetime.utcnow().isoformat()
        if not refresh_date.endswith('Z'):
            refresh_date = "%sZ" % refresh_date

        bunch = []
        for item in reader:
            doc = {
                'id': 'hdfs://hdfs:54310%s' % item['file_path'],
                'refresh_date_dt': refresh_date,
                'file_path_s': item['file_path'],
                'file_size_l': item['file_size'],
                'file_ext_s': item['file_ext'],
                'file_name_s': item['file_name'],
                'permissions_s': item['permissions'],
                'hdfs_replicas_i': item['number_of_replicas'],
                'hdfs_user_s': item['user_id'],
                'hdfs_group_s': item['group_id'],
                'modified_at_dt': "%sZ" % item['modified_at'],
                'timestamp_dt': "%sZ" % item['timestamp'],
                'year_i': item['timestamp'][0:4],
                'recognised_b': item['recognised'],
                'kind_s': item['kind'],
                'collection_s': item['collection'],
                'stream_s': item['stream'],
                'job_s': item['job'],
                'layout_s': item['layout']
            }
            bunch.append(doc)
            if len(bunch) >= self.batch_size:
                self.total += len(bunch)
                yield bunch
                bunch = []

        if len(bunch) > 0:
            self.total += len(bunch)
            yield bunch

    def run(self):
        print("Connect")
        # Set up a connection for this:
        solr = pysolr.Solr(self.trackdb, always_commit=False)

        # Clear first if configured:
        if self.clear_trackdb:
            solr.delete(q='*:*', commit=False)

        # Go through the data and assemble the resources for each crawl:
        self.total = 0
        print("open up")
        fields = {}
        with self.input().open('r') as fin:
            reader = csv.DictReader(fin)
            for bunch in self.entry_generator(reader):
                # Generate the list of fields up update (avoid replacing whole document)
                if len(fields) == 0:
                    for key in bunch[0]:
                        if key != 'id':
                            fields[key] = 'set'
                # Perform the update, commit within 30 seconds please:
                solr.add(bunch, fieldUpdates=fields, commitWithin="30000")
                logger.info("Posted %i records..." % self.total)

        # And make it visible:
        solr.commit()

        # FIXME also check last_seen_at date and warn if anything appears to have gone missing?

        # Sanity check:
        if self.total == 0:
            raise Exception("No filenames generated! Something went wrong!")

        # Record we completed successfully:
        self.output().touch()


class ListEmptyFiles(luigi.Task):
    """
    Takes the full file list and extracts the empty files, as these should be checked.
    """
    date = luigi.DateParameter(default=datetime.date.today())
    task_namespace = "analyse.hdfs"

    def requires(self):
        return ListParsedPaths(self.date)

    def output(self):
        return state_file(self.date, 'hdfs', 'empty-files-list.csv')

    def run(self):
        with self.input().open('r') as fin:
            reader = csv.DictReader(fin)
            with self.output().open('w') as fout:
                # Set up output file:
                writer = csv.DictWriter(fout, fieldnames=reader.fieldnames)
                writer.writeheader()
                for item in reader:
                    # Archive file names:
                    if not item['permissions'].startswith('d') and item['file_size'] == "0":
                        writer.writerow(item)


class ListDuplicateFiles(luigi.Task):
    """
    List all files on HDFS that appear to be duplicates.
    """
    date = luigi.DateParameter(default=datetime.date.today())
    task_namespace = "analyse.hdfs"

    total_unduplicated = 0
    total_duplicated = 0

    def requires(self):
        return ListParsedPaths(self.date)

    def output(self):
        return state_file(self.date, 'hdfs', 'duplicate-files-list.tsv')

    def run(self):
        filenames = {}
        with self.input().open('r') as fin:
            reader = csv.DictReader(fin)
            for item in reader:
                # Archive file names:
                basename = os.path.basename(item['file_name'])
                if basename not in filenames:
                    filenames[basename] = [item['file_name']]
                else:
                    filenames[basename].append(item['file_name'])

        # And emit duplicates:
        self.total_duplicated = 0
        self.total_unduplicated = 0
        with self.output().open('w') as f:
            for basename in filenames:
                if len(filenames[basename]) > 1:
                    self.total_duplicated += 1
                    f.write("%s\t%i\t%s\n" % (basename, len(filenames[basename]), json.dumps(filenames[basename])))
                else:
                    self.total_unduplicated += 1
        logger.info("Of %i WARC filenames, %i are stored in a single HDFS location." % (len(filenames), self.total_unduplicated))


class ListByCrawl(luigi.Task):
    """
    Identifies in the crawl output files and arranges them by crawl.
    """
    date = luigi.DateParameter(default=datetime.date.today())

    task_namespace = "analyse.report"

    totals = {}
    collections = {}

    def requires(self):
        return ListParsedPaths(self.date)

    #def complete(self):
    #    return False

    def output(self):
        return state_file(self.date, 'hdfs', 'crawl-file-lists.txt')

    def run(self):
        # Go through the data and assemble the resources for each crawl:
        crawls = { }
        unparsed = []
        unparsed_dirs = set()
        with self.input().open('r') as fin:
            reader = csv.DictReader(fin)

            for p in reader:
                # Process this line:
                collection = 'no-collection'
                stream = 'no-stream'

                # Store the job details:
                if p['recognised'] and p['job']:
                    if p['job'] not in crawls:
                        crawls[p['job']] = {}
                    if p['launch'] not in crawls[p['job']]:
                        crawls[p['job']][p['launch']] = {}
                    # Store the launch data:
                    if p['launch_datetime']:
                        crawls[p['job']][p['launch']]['date'] = p['launch_datetime'].isoformat()
                        crawls[p['job']][p['launch']]['launch_datetime'] = p['launch_datetime'].isoformat()
                        launched = p['launch_datetime'].strftime("%d %b %Y")
                    else:
                        launched = '?'
                    crawls[p['job']][p['launch']]['stream'] = p['stream']
                    crawls[p['job']][p['launch']]['tags'] = ['crawl-%s' % p['stream'].name, 'crawl-%s-%s' % (p['stream'].name, p['job'])]
                    crawls[p['job']][p['launch']]['total_files'] = 0

                    # Determine the collection and store information at that level:
                    if p['stream'] == 'frequent' or p['stream'] == 'domain':
                        collection = 'npld'
                        crawls[p['job']][p['launch']]['categories'] = ['legal-deposit crawls', '%s crawl' % p['job'].split('-')[0]]
                        crawls[p['job']][p['launch']]['title'] = "NPLD %s crawl, launched %s" % (p['job'], launched)
                    elif p['stream'] == 'selective':
                        collection = 'selective'
                        crawls[p['job']][p['launch']]['categories'] = ['selective crawls',
                                                                 '%s crawl' % p['job'].split('-')[0]]
                        crawls[p['job']][p['launch']]['title'] = "Selective %s crawl, launched %s" % (p['job'], launched)

                    # Append this item:
                    if 'files' not in crawls[p['job']][p['launch']]:
                        crawls[p['job']][p['launch']]['files'] = []
                    file_info = {
                        'path': p['file_path'],
                        'kind': p['kind'],
                        'timestamp': p['timestamp_datetime'].isoformat(),
                        'filesize': p['file_size'],
                        'modified_at': p['modified_at']
                    }
                    crawls[p['job']][p['launch']]['files'].append(file_info)
                    crawls[p['job']][p['launch']]['total_files'] += 1

                if not p['recognised']:
                    #logger.warning("Could not parse: %s" % item['filename'])
                    unparsed.append(p['file_name'])
                    unparsed_dirs.add(os.path.dirname(p['file_name']))

                # Also count up files and bytes:
                if p['stream']:
                    stream = p['stream'].name
                if stream not in self.totals:
                    self.totals[stream] = {}
                    self.totals[stream]['all'] = {'count': 0, 'bytes': 0}
                    self.collections[stream] = collection
                # Totals for all files:
                self.totals[stream]['all']['count'] += 1
                self.totals[stream]['all']['bytes'] += int(p['file_size'])
                # Totals broken down by kind:
                if p['kind'] not in self.totals[stream]:
                    self.totals[stream][p['kind']] = { 'count': 0, 'bytes': 0}
                self.totals[stream][p['kind']]['count'] += 1
                self.totals[stream][p['kind']]['bytes'] += int(p['file_size'])

        # Now emit a file for each, remembering the filenames as we go:
        filenames = []
        for job in crawls:
            for launch in crawls[job]:
                # Grab the stream and just use the name in the dict so we can serialise to JSON:
                stream = crawls[job][launch]['stream']
                crawls[job][launch]['stream'] = stream.name
                # Output a Package file ('versioned' by file count):
                outfile = CrawlPackageTarget(stream, job, launch, crawls[job][launch]['total_files'])
                with outfile.open('w') as f:
                    f.write(json.dumps(crawls[job][launch], indent=2, sort_keys=True))
                filenames.append(outfile.path)
                # Output a Crawl Report file (always the latest version):
                outfile = CrawlReportTarget(stream, job, launch)
                with outfile.open('w') as f:
                    f.write(json.dumps(crawls[job][launch], indent=2, sort_keys=True))
                filenames.append(outfile.path)

        # Output the totals:
        outfile = ReportTarget('data/crawls', 'totals.json')
        with outfile.open('w') as f:
            totals = {
                'totals': self.totals,
                'collections': self.collections
            }
            f.write(json.dumps(totals, indent=2, sort_keys=True))

        # Go through the data and generate some summary stats:
        stats = {}

        # Also emit a list of files that could not be understood:
        outfile = ReportTarget('data/crawls', 'unparsed-file-paths.json')
        with outfile.open('w') as f:
            unparsed_data = {
                'folders': sorted(list(unparsed_dirs)),
                # 'files': unparsed,
                'num_files': len(unparsed)
            }
            f.write(json.dumps(unparsed_data, indent=2, sort_keys=True))

        # Sanity check:
        if len(filenames) == 0:
            raise Exception("No filenames generated! Something went wrong!")

        # Finally, emit the list of output files as the task output:
        with self.output().open('w') as f:
            for output_path in filenames:
                f.write('%s\n' % output_path)

    def get_metrics(self, registry):
        # type: (CollectorRegistry) -> None

        g_b = Gauge('ukwa_files_total_bytes',
                  'Total size of files on HDFS in bytes.',
                  labelnames=['collection', 'stream', 'kind'], registry=registry)
        g_c = Gauge('ukwa_files_total_count',
                  'Total number of files on HDFS.',
                  labelnames=['collection', 'stream', 'kind'], registry=registry)

        # Go through the kinds of data in each collection and
        for stream in self.totals:
            col = self.collections[stream]
            for kind in self.totals[stream]:
                g_b.labels(collection=col, stream=stream, kind=kind).set(self.totals[stream][kind]['bytes'])
                g_c.labels(collection=col, stream=stream, kind=kind).set(self.totals[stream][kind]['count'])


class GenerateHDFSSummaries(luigi.WrapperTask):
    """
    A 'Wrapper Task' that invokes the summaries of HDFS we are interested in.
    """
    task_namespace = "analyse.report"

    def requires(self):
        return [ ListDuplicateFiles(), ListEmptyFiles(), ListByCrawl(), ListParsedPaths() ]


if __name__ == '__main__':
    import logging

    logging.getLogger().setLevel(logging.INFO)
    #luigi.run(['analyse.hdfs.ListParsedPaths', '--local-scheduler', '--date', '2018-02-12'])
    luigi.run(['analyse.hdfs.UpdateWarcsDatabase', '--local-scheduler', '--date', '2018-02-12'])
