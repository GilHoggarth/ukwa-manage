#!/bin/bash

##############################################################################################
# Intended to run hourly. Checks whether all WARCs created in the last 2 hours exist in HDFS #
# at the same location.                                                                      #
# If not, they are copied and a MapReduce job run to CDX-index them.                         #
# A full CDX can be created by sorting all the outputs.                                      #
##############################################################################################

HDFS_CDX_ROOT=/heritrix/output/wayback/cdx-index
ROOT=/heritrix/output
HOOP="http://dls.httpfs.wa.bl.uk:14000/webhdfs/v1"
SUFFIX="?user.name=hadoop&op=GETFILESTATUS"
TMP=/tmp
JAR=/heritrix/bin/warc-hadoop-indexer-1.1.1-SNAPSHOT-job.jar
CLASS=uk.bl.wa.hadoop.mapreduce.cdx.ArchiveCDXGenerator
SPLIT="/heritrix/output/wayback/split.txt"
RED=1

log()
{
        echo "[$(date "+%Y-%m-%d %H:%M:%S")] $1"
}

copy_warc()
{
	log "Copying $1 to HDFS..."
	hadoop fs -put "$1" "$1"
}

del_warc()
{
	log "Deleting hdfs://$1..."
	hadoop fs -rm "$1"
}

COUNT=0
OUTPUT="$(date +%Y%m%d%H%M%S)"
JOBS=()
while read warc
do
	TEST="$(curl --silent "$HOOP$warc$SUFFIX")"
	if [[ "$TEST" =~ "FileNotFoundException" ]]
	then
		SUCCESS=0
		while [[ $SUCCESS == 0 ]]
		do
			copy_warc "$warc"
			#Verify that the copy succeeded.
			TEST="$(curl --silent "$HOOP$warc$SUFFIX")"
			HDFS_LENGTH=$(curl --silent "$HOOP$warc$SUFFIX" | sed -r 's@^.+length":([0-9]+),".+$@\1@')
			DISK_LENGTH=$(stat -Lc %s "$warc")
			if [[ "$TEST" =~ "FileNotFoundException" ]] || [[ $HDFS_LENGTH -ne $DISK_LENGTH ]]
			then
				log "ERROR: Problem copying $warc"
				del_warc "$warc"
			else
				SUCCESS=1
			fi
		done
		[[ "$warc" =~ "viral" ]] || echo "$warc" >> $TMP/$OUTPUT.job
		COUNT=$((COUNT+1))
		JOBS+=("$(echo "$warc" | cut -d "/" -f5)")
	fi
done < <(find $ROOT/ -mtime -2 -name "*.warc.gz" 2> /dev/null)
JOBS=($(printf "%q\n" "${JOBS[@]}" | sort -u))

if [[ -e $TMP/$OUTPUT.job ]] && [[ $(wc -l $TMP/$OUTPUT.job | awk '{ print $1 }') -gt 0 ]]
then
	log "Generating CDX for $OUTPUT.job in $HDFS_CDX_ROOT/$OUTPUT/..."
	hadoop fs -put $TMP/$OUTPUT.job $TMP/$OUTPUT.job && rm $TMP/$OUTPUT.job
	hadoop jar $JAR $CLASS -Dmapred.compress.map.output=true -i $TMP/$OUTPUT.job -o $HDFS_CDX_ROOT/$OUTPUT/ -s $SPLIT -h -r $RED &
fi

if [[ $COUNT -eq 0 ]]
then
	exit 1
else
	log "Complete. $COUNT files copied."
	exit 0
fi
